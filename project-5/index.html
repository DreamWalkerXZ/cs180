<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css"
        integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"
        integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg"
        crossorigin="anonymous"></script>
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <title>Project 5: Fun With Diffusion Models!</title>
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
</head>

<body class="container mx-auto p-2">
    <h1 class="text-3xl font-bold text-center">Fun With Diffusion Models!</h1>
    <p class="text-2xl text-center p-2">
        Yueheng Zeng
        <span class="font-bold text-rose-500">@</span>
        Project 5
    </p>
    <h2 class="text-3xl font-bold p-2">Overview</h2>
    <p class="text-xl p-2">
        This project focuses on implementing a diffusion model to generate images. The diffusion model is a generative
        model that generates images by iteratively applying a series of transformations to a noise image. The model is
        trained to generate images that are similar to the training data. The project is divided into two parts. In Part
        1, we implement the forward process of the diffusion model and explore different denoising techniques using a
        pre-trained diffusion model. In Part 2, we train a single-step denoising UNet and extend it to include
        time-conditioning and class-conditioning.
    </p>

    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/2.1 Course Logo Upsampled.png" class="h-72">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Course Logo Generated and Upsampled by Diffusion Model
    </p>

    <h2 class="text-3xl font-bold p-2">Table of Contents</h2>
    <ol class="text-xl pl-9 list-disc p-2 text-blue-500">
        <li>
            <a class="underline" href="#a0">
                5A Part 0: Setup
            </a>
        </li>
        <li>
            <a class="underline" href="#a11">
                5A Part 1.1: Implementing the Forward Process
            </a>
        </li>
        <li>
            <a class="underline" href="#a12">
                5A Part 1.2: Classical Denoising
            </a>
        </li>
        <li>
            <a class="underline" href="#a13">
                5A Part 1.3: One-Step Denoising
            </a>
        </li>
        <li>
            <a class="underline" href="#a14">
                5A Part 1.4: Iterative Denoising
            </a>
        </li>
        <li>
            <a class="underline" href="#a15">
                5A Part 1.5: Diffusion Model Sampling
            </a>
        </li>
        <li>
            <a class="underline" href="#a16">
                5A Part 1.6: Classifier-Free Guidance (CFG)
            </a>
        </li>
        <li>
            <a class="underline" href="#a17">
                5A Part 1.7: Image-to-image Translation
            </a>
        </li>
        <li>
            <a class="underline" href="#a171">
                5A Part 1.7.1: Editing Hand-Drawn and Web Images
            </a>
        </li>
        <li>
            <a class="underline" href="#a172">
                5A Part 1.7.2: Inpainting
            </a>
        </li>
        <li>
            <a class="underline" href="#a173">
                5A Part 1.7.3: Text-Conditional Image-to-image Translation
            </a>
        </li>
        <li>
            <a class="underline" href="#a18">
                5A Part 1.8: Visual Anagrams
            </a>
        </li>
        <li>
            <a class="underline" href="#a19">
                5A Part 1.9: Hybrid Images
            </a>
        </li>
        <li>
            <a class="underline" href="#abw">
                5A Bells & Whistles: A Course Logo
            </a>
        </li>
        <li>
            <a class="underline" href="#b1">
                5B Part 1: Training a Single-Step Denoising UNet
            </a>
        </li>
        <li>
            <a class="underline" href="#b21">
                5B Part 2.1: Adding Time-Conditioning to UNet
            </a>
        </li>
        <li>
            <a class="underline" href="#b24">
                5B Part 2.4: Adding Class-Conditioning to UNet
            </a>
        </li>
        <li>
            <a class="underline" href="#bbw">
                5B Bells & Whistles: Sampling Gifs
            </a>
        </li>
    </ol>

    <h2 id="1" class="text-3xl font-bold p-2">5A Part 0: Setup</h2>
    <p class="text-xl p-2">
        In this part, we set up the environment and load the pre-trained diffusion model to generate images using seed
        180.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/0 num_inference_steps=20.png" class="h-48">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Generated Images with num_inference_steps=20
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/0 num_inference_steps=40.png" class="h-48">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Generated Images with num_inference_steps=40
    </p>
    <p class="text-xl p-2">
        We can see the quality of the generated images are quite good. They are highly correlated with the text prompts.
        However, there are some artifacts in the images.

        After increasing the number of inference steps, there is actually not much difference in the quality of the
        images. The
        defects in the images are still present.
    </p>

    <h2 id="a11" class="text-3xl font-bold p-2">5A Part 1.1: Implementing the Forward Process</h2>
    <p class="text-xl p-2">
        In this part, we implement the forward process of the diffusion model.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 250.png" class="h-48">
        <img src="output/a/1.1 t = 500.png" class="h-48 pl-6">
        <img src="output/a/1.1 t = 750.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Generated Images at t = 250, 500, and 750
    </p>

    <h2 id="a12" class="text-3xl font-bold p-2">5A Part 1.2: Classical Denoising</h2>
    <p class="text-xl p-2">
        In this part, we use the classical Gaussian blur filter to denoise the images.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 250.png" class="h-48">
        <img src="output/a/1.2 t = 250.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Noisy Image at t = 250 with Gaussian Blur Denoising
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 500.png" class="h-48">
        <img src="output/a/1.2 t = 500.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Noisy Image at t = 500 with Gaussian Blur Denoising
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 750.png" class="h-48">
        <img src="output/a/1.2 t = 750.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Noisy Image at t = 750 with Gaussian Blur Denoising
    </p>

    <h2 id="a13" class="text-3xl font-bold p-2">5A Part 1.3: One-Step Denoising</h2>
    <p class="text-xl p-2">
        In this part, we use the pre-trained diffusion model to denoise the images with one step.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 250.png" class="h-48">
        <img src="output/a/1.3 t = 250.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Noisy Image at t = 250 with One-Step Denoising
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 500.png" class="h-48">
        <img src="output/a/1.3 t = 500.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Noisy Image at t = 500 with One-Step Denoising
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.1 t = 750.png" class="h-48">
        <img src="output/a/1.3 t = 750.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Noisy Image at t = 750 with One-Step Denoising
    </p>

    <h2 id="a14" class="text-3xl font-bold p-2">5A Part 1.4: Iterative Denoising</h2>
    <p class="text-xl p-2">
        In this part, we iteratively denoise the images with the pre-trained diffusion model.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.4 t = 690.png" class="h-48">
        <img src="output/a/1.4 t = 540.png" class="h-48 pl-6">
        <img src="output/a/1.4 t = 390.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Predicted Noisy Images at t = 690, 540, and 390 with Iterative Denoising
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.4 t = 240.png" class="h-48">
        <img src="output/a/1.4 t = 90.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Predicted Noisy Images at t = 240, 90 with Iterative Denoising
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/Test Image.png" class="h-48">
        <img src="output/a/1.4 Iterative Denoise.png" class="h-48 pl-6">
        <img src="output/a/1.3 t = 750.png" class="h-48 pl-6">
        <img src="output/a/1.2 t = 750.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Test Image, Iterative Denoised Image, One-Step Denoised Image, and Gaussian Blur Denoised image
    </p>

    <h2 id="a15" class="text-3xl font-bold p-2">5A Part 1.5: Diffusion Model Sampling</h2>
    <p class="text-xl p-2">
        In this part, we sample images from the diffusion model.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.5 Generated Image 0.png" class="h-48">
        <img src="output/a/1.5 Generated Image 1.png" class="h-48 pl-6">
    </div>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.5 Generated Image 2.png" class="h-48">
        <img src="output/a/1.5 Generated Image 3.png" class="h-48 pl-6">
        <img src="output/a/1.5 Generated Image 4.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Generated Images
    </p>

    <h2 id="a16" class="text-3xl font-bold p-2">5A Part 1.6: Classifier-Free Guidance (CFG)</h2>
    <p class="text-xl p-2">
        In this part, we use the classifier-free guidance to guide the diffusion model to generate images with the
        prompt "a high quality photo".
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.6 Generated Image with CFG 0.png" class="h-48">
        <img src="output/a/1.6 Generated Image with CFG 1.png" class="h-48 pl-6">
    </div>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.6 Generated Image with CFG 2.png" class="h-48">
        <img src="output/a/1.6 Generated Image with CFG 3.png" class="h-48 pl-6">
        <img src="output/a/1.6 Generated Image with CFG 4.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Generated Images with CFG
    </p>
    <p class="text-xl p-2">
        We can see that the generated images have higher quality than the images generated without CFG.
    </p>

    <h2 id="a17" class="text-3xl font-bold p-2">5A Part 1.7: Image-to-image Translation</h2>
    <p class="text-xl p-2">
        In this part, we're going to take the original test image, noise it a little, and force it back onto the image
        manifold without any
        conditioning.
    </p>
    <p class="text-center text-xl p-2">
        <b>Campenile</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7 i_start = 1.png" class="h-48">
        <img src="output/a/1.7 i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7 i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7 i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7 i_start = 10.png" class="h-48">
        <img src="output/a/1.7 i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7 i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7 Original Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with i_start = 10, 20, 30, and Original Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Mong Kok</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7 Mong Kok i_start = 1.png" class="h-48">
        <img src="output/a/1.7 Mong Kok i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7 Mong Kok i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7 Mong Kok i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Mong Kok and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7 Mong Kok i_start = 10.png" class="h-48">
        <img src="output/a/1.7 Mong Kok i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7 Mong Kok i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7 Original Mong Kok Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Mong Kok and i_start = 10, 20, 30, and Original Mong Kok Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Victoria Harbour</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7 Victoria Harbour i_start = 1.png" class="h-48">
        <img src="output/a/1.7 Victoria Harbour i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7 Victoria Harbour i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7 Victoria Harbour i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Victoria Harbour and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7 Victoria Harbour i_start = 10.png" class="h-48">
        <img src="output/a/1.7 Victoria Harbour i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7 Victoria Harbour i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7 Original Victoria Harbour Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Victoria Harbour and i_start = 10, 20, 30, and Original Victoria Harbour Image
    </p>

    <h2 id="a171" class="text-3xl font-bold p-2">5A Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
    <p class="text-xl p-2">
        In this part, we're going to apply the SDEdit to nonrealistic images (e.g. painting, a sketch, some scribbles),
        making them look more realistic.
    </p>
    <p class="text-center text-xl p-2">
        <b>Hand-Drawn Image: Merry Cat-mas!</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.1 Catmas i_start = 1.png" class="h-48">
        <img src="output/a/1.7.1 Catmas i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Catmas i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Catmas i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Merry Cat-mas! and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.1 Catmas i_start = 10.png" class="h-48">
        <img src="output/a/1.7.1 Catmas i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Catmas i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Catmas Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Merry Cat-mas! and i_start = 10, 20, 30, and Original Merry Cat-mas! Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Hand-Drawn Image: Monster</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.1 Monster i_start = 1.png" class="h-48">
        <img src="output/a/1.7.1 Monster i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Monster i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Monster i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Monster and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.1 Monster i_start = 10.png" class="h-48">
        <img src="output/a/1.7.1 Monster i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Monster i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Monster Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Monster and i_start = 10, 20, 30, and Original Monster Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Web Image: <a href="https://www.minecraft.net/en-us/about-minecraft"
                class="underline text-blue-500">Minecraft</a></b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.1 Minecraft i_start = 1.png" class="h-48">
        <img src="output/a/1.7.1 Minecraft i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Minecraft i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Minecraft i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Minecraft and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.1 Minecraft i_start = 10.png" class="h-48">
        <img src="output/a/1.7.1 Minecraft i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Minecraft i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7.1 Minecraft Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with Minecraft and i_start = 10, 20, 30, and Original Minecraft Image
    </p>

    <h2 id="a172" class="text-3xl font-bold p-2">5A Part 1.7.2: Inpainting</h2>
    <p class="text-xl p-2">
        In this part, we're going to implement inpainting. To do this, we can run the diffusion denoising loop. But at
        every step, after obtaining the denoised image, we can force the pixels that we do not want to change back to
        the original image.
    </p>
    <p class="text-center text-xl p-2">
        <b>Campanile</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.2 Original Image.png" class="h-48">
        <img src="output/a/1.7.2 Mask.png" class="h-48 pl-6">
        <img src="output/a/1.7.2 Inpainted Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Original Image, Mask, and Inpainted Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Street</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.2 Original Street Image.png" class="h-48">
        <img src="output/a/1.7.2 Street Mask.png" class="h-48 pl-6">
        <img src="output/a/1.7.2 Inpainted Street Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Original Street Image, Street Mask, and Inpainted Street Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Victoria Harbour</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.2 Original Victoria Harbour Image.png" class="h-48">
        <img src="output/a/1.7.2 Victoria Harbour Mask.png" class="h-48 pl-6">
        <img src="output/a/1.7.2 Inpainted Victoria Harbour Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Original Victoria Harbour Image, Victoria Harbour Mask, and Inpainted Victoria Harbour Image
    </p>

    <h2 id="a173" class="text-3xl font-bold p-2">5A Part 1.7.3: Text-Conditional Image-to-image Translation</h2>
    <p class="text-xl p-2">
        In this part, we will do the same thing as SDEdit, but guide the projection with a text prompt. This is no
        longer pure "projection to
        the natural image manifold" but also adds control using language.
    </p>
    <p class="text-center text-xl p-2">
        <b>Campanile -> "a rocket ship"</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.3 i_start = 1.png" class="h-48">
        <img src="output/a/1.7.3 i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with "a rocket ship" and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.3 i_start = 10.png" class="h-48">
        <img src="output/a/1.7.3 i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Original Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with "a rocket ship" and i_start = 10, 20, 30, and Original Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Octocat -> "a photo of a dog"</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.3 Octocat i_start = 1.png" class="h-48">
        <img src="output/a/1.7.3 Octocat i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Octocat i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Octocat i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with "a photo of a dog" and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.3 Octocat i_start = 10.png" class="h-48">
        <img src="output/a/1.7.3 Octocat i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Octocat i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Original Octocat Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with "a photo of a dog" and i_start = 10, 20, 30, and Original Octocat Image
    </p>
    <p class="text-center text-xl p-2">
        <b>Hoover Tower -> "a rocket ship"</b>
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.3 Hoover Tower i_start = 1.png" class="h-48">
        <img src="output/a/1.7.3 Hoover Tower i_start = 3.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Hoover Tower i_start = 5.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Hoover Tower i_start = 7.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with "a rocket ship" and i_start = 1, 3, 5, 7
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.7.3 Hoover Tower i_start = 10.png" class="h-48">
        <img src="output/a/1.7.3 Hoover Tower i_start = 20.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Hoover Tower i_start = 30.png" class="h-48 pl-6">
        <img src="output/a/1.7.3 Original Hoover Tower Image.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        SDEdit with "a rocket ship" and i_start = 10, 20, 30, and Original Hoover Tower Image
    </p>

    <h2 id="a18" class="text-3xl font-bold p-2">5A Part 1.8: Visual Anagrams</h2>
    <p class="text-xl p-2">
        In this part, we are going to implement <a href="https://dangeng.github.io/visual_anagrams/"
            class="underline text-blue-500">Visual Anagrams</a>. We
        will create an image that looks like something else when flipped upside down. The only modification to the
        original iterative denoising is that we will calculate \(\epsilon\) as follows:
        \[
        \epsilon_1 = \text{UNet}(x_t, t, p_1)
        \]
        \[
        \epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2))
        \]
        \[
        \epsilon = (\epsilon_1 + \epsilon_2) / 2
        \]
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.8 An Oil Painting of People Around a Campfire.png" class="h-48">
        <img src="output/a/1.8 An Oil Painting of an Old Man.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        An Oil Painting of People Around a Campfire and An Oil Painting of an
        Old Man
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.8 An Oil Painting of a Snowy Mountain Village.png" class="h-48">
        <img src="output/a/1.8 An Oil Painting of People Around a Campfire 2.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        An Oil Painting of a Snowy Mountain Village and An Oil Painting of People Around a Campfire
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.8 A Dreamy Oil Painting of a Crescent Moon Cradling a Sleeping Figure.png" class="h-48">
        <img src="output/a/1.8 An Ocean Wave Crashing Against a Lighthouse.png" class="h-48 pl-6">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        A Dreamy Oil Painting of a Crescent Moon Cradling a Sleeping Figure and An Ocean Wave Crashing Against a
        Lighthouse
    </p>

    <h2 id="a19" class="text-3xl font-bold p-2">5A Part 1.9: Hybrid Images</h2>
    <p class="text-xl p-2">
        In this part, we are going to implement <a href="https://arxiv.org/abs/2404.11615"
            class="underline text-blue-500">Factorized Diffusion</a> and create hybrid images that look like one thing
        up close and another thing from far away. The only modification to the original iterative denoising is that we
        will calculate \(\epsilon\) as follows:
        \[
        \epsilon_1 = \text{UNet}(x_t, t, p_1)
        \]
        \[
        \epsilon_2 = \text{UNet}(x_t, t, p_2)
        \]
        \[
        \epsilon = f_\text{lowpass}(\epsilon_1) + f_\text{highpass}(\epsilon_2)
        \]
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.9 Hybrid Lithograph of a Skull and Waterfalls.png" class="h-48">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Hybrid Lithograph of a Skull (Low) and Waterfalls (High)
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.9 Hybrid Image of an Ancient Clock Face and Historical Moments.png" class="h-48">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Hybrid Image of an Ancient Clock Face (Low) and Historical Moments (High)
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/1.9 Hybrid Oil Painting of an Old Man and a Snowy Mountain Village.png" class="h-48">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Hybrid Oil Painting of an Old Man (Low) and a Snowy Mountain Village (High)
    </p>

    <h2 id="abw" class="text-3xl font-bold p-2">5A Bells & Whistles: A Course Logo</h2>
    <p class="text-xl p-2">
        In this part, we are going to design a course logo using the diffusion model with prompt "A man whose head is a
        camera of brand CS180".
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/a/2.1 Course Logo Upsampled.png" class="h-48">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Course Logo (Upsampled)
    </p>
    <p class="text-xl p-2">
        The man in the logo looks cool! However, the CS180 brand is not on the camera (it might be caused by the word
        CS180 is not shown in the training data).
    </p>

    <h2 id="b1" class="text-3xl font-bold p-2">5B Part 1: Training a Single-Step Denoising UNet</h2>
    <p class="text-xl p-2">
        In this part, we are going to train a single-step denoising UNet to denoise the digits in the MNIST dataset.
        Firstly, we will need to implement the noising process defined as follows:
        \[
        z = x + \sigma \epsilon,\quad \text{where }\epsilon \sim N(0, I)
        \]
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/1.2 Varying levels of noise on MNIST digits.png" class="h-96">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Varying levels of noise on MNIST digits
    </p>
    <p class="text-xl p-2">
        Then we will train a single-step denoising UNet to denoise the noisy digits at \(\sigma = 0.5\).
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/1.2 Training Loss per Batch.png" class="h-96">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Training Loss per Batch
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/1.2.1 Results on digits from the test set after 1 epochs of training.png" class="h-96">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Results on digits from the test set after 1 epochs of training
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/1.2.1 Results on digits from the test set after 5 epochs of training.png" class="h-96">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Results on digits from the test set after 5 epochs of training
    </p>
    <p class="text-xl p-2">
        We can see that the denoising UNet can denoise the noisy digits effectively after 5 epochs of training. However,
        what if we let it denoise the digits with different levels of noise that it was not trained on?
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/1.2.2 Results on digits from the test set with varying noise levels.png" class="w-full">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Results on digits from the test set with varying noise levels
    </p>
    <p class="text-xl p-2">
        We can see that the denoising UNet cannot denoise the digits effectively with noise levels that it was not
        trained on, especially when the noise level is high.
    </p>

    <h2 id="b21" class="text-3xl font-bold p-2">5B Part 2.1: Adding Time-Conditioning to UNet</h2>
    <p class="text-xl p-2">
        In this part, we are going to add time-conditioning to the UNet, making it a diffusion model. Firstly, we will
        add the noise with the following equation:
        \[
        x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon
        \quad \text{where}~ \epsilon \sim N(0, 1)
        \]
        And our objective is to minimize the following loss function:
        \[
        L = \mathbb{E}_{\epsilon,x_0,t} \|\epsilon_{\theta}(x_t, t) -
        \epsilon\|^2
        \]
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/2.2 Training Loss per Batch.png" class="h-96">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Training Loss per Batch
    </p>
    <p class="text-xl p-2">
        After training the diffusion model, we can sample high-quality digits from the model iteratively.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <table class="show_videos" style="border-spacing:0px;">
            <tr>
                <td style="padding:1px;">
                    <div style="display:flex; align-items:left;">
                        <div style="display:flex; flex-direction:column; align-items:center;">
                            <div>After 5 epochs</div>
                            <div><video controls width="280" height="112" style="object-fit:cover;" loop autoplay muted>
                                    <source src="output/b/2.3 After 5 epochs.mp4" type="video/mp4">
                                </video></div>
                        </div>
                    </div>
                </td>
                <td style="padding:16px;">
                    <div style="display:flex; align-items:left;">
                        <div style="display:flex; flex-direction:column; align-items:center;">
                            <div>After 20 epochs</div>
                            <div><video controls width="280" height="112" style="object-fit:cover;" loop autoplay muted>
                                    <source src="output/b/2.3 After 20 epochs.mp4" type="video/mp4">
                                </video></div>
                        </div>
                    </div>
                </td>
            </tr>
        </table>
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Sampling digits from time-conditioned UNet after 5 and 20 epochs iteratively
    </p>

    <h2 id="b24" class="text-3xl font-bold p-2">5B Part 2.4: Adding Class-Conditioning to UNet</h2>
    <p class="text-xl p-2">
        In this part, we are going to add class-conditioning to the UNet, enabling us to specify the which digit we want
        to generate.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <img src="output/b/2.4.1 Training Loss per Batch.png" class="h-96">
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Training Loss per Batch
    </p>
    <p class="text-xl p-2">
        After training the class-conditioned diffusion model, we choose which digit to generate and sample high-quality
        digits from the model iteratively.
    </p>
    <div class="flex justify-center p-2 flex-wrap">
        <table class="show_videos" style="border-spacing:0px;">
            <tr>
                <td style="padding:1px;">
                    <div style="display:flex; align-items:left;">
                        <div style="display:flex; flex-direction:column; align-items:center;">
                            <div>After 5 epochs</div>
                            <div><video controls width="280" height="112" style="object-fit:cover;" loop autoplay muted>
                                    <source src="output/b/2.5 After 5 epochs.mp4" type="video/mp4">
                                </video></div>
                        </div>
                    </div>
                </td>
                <td style="padding:16px;">
                    <div style="display:flex; align-items:left;">
                        <div style="display:flex; flex-direction:column; align-items:center;">
                            <div>After 20 epochs</div>
                            <div><video controls width="280" height="112" style="object-fit:cover;" loop autoplay muted>
                                    <source src="output/b/2.5 After 20 epochs.mp4" type="video/mp4">
                                </video></div>
                        </div>
                    </div>
                </td>
            </tr>
        </table>
    </div>
    <p class="text-center text-xl p-2 text-gray-500">
        Sampling digits from class-conditioned UNet after 5 and 20 epochs iteratively
    </p>

    <h2 id="bbw" class="text-3xl font-bold p-2">5B Bells & Whistles: Sampling Gifs</h2>
    <p class="text-xl p-2">
        This part has already been completed in the previous parts.
    </p>

</body>

</html>